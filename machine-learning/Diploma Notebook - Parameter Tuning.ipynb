{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    \"\"\"Набор классификаторов с параметрами\"\"\"\n",
    "    \n",
    "    classifiers = {\n",
    "        \"LogRer\": (LogisticRegression, ParameterGrid({\n",
    "            'random_state': [RANDOM_SEED],\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'penalty': ['l1', 'l2']\n",
    "            })),\n",
    "        \"RandForest\": (RandomForestClassifier, ParameterGrid({\n",
    "            'random_state': [RANDOM_SEED],\n",
    "            'n_estimators': [10, 100, 200, 400, 600, 800, 1000],\n",
    "            'max_depth': [3, 4, 6, 8]\n",
    "        })),\n",
    "        \"XGBoost\": (XGBClassifier, ParameterGrid({\n",
    "            'random_state': [RANDOM_SEED],\n",
    "            \"eta\": [0.03, 0.1, 0.3],\n",
    "            \"n_estimators\": [100, 200, 400, 800, 1000],\n",
    "            \"max_depth\": [3, 4, 6, 8],\n",
    "            \"logging_level\": ['Silent'],\n",
    "            'subsample': [0.7, 0.85, 1]\n",
    "        }))\n",
    "    }\n",
    "    \n",
    "    for clf_name in classifiers:\n",
    "        for params in classifiers[clf_name][1]:\n",
    "            yield clf_name, params, classifiers[clf_name][0](**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_best_classifier(X, y, kfolds=10, valid=0.2):\n",
    "    \"\"\"Выбор лучшего классификатора\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    LOG_FILE = 'logs.log'\n",
    "    \n",
    "    stats = np.array([])\n",
    "    row_names = []\n",
    "    column_names = [\n",
    "        'train_acc', 'train_auc', 'train_ll', 'test_acc', 'test_auc', 'test_ll',\n",
    "        'valid_acc', 'valid_auc', 'valid_ll', 'time'\n",
    "    ]\n",
    "    \n",
    "    best_clf_name = None\n",
    "    best_params = None\n",
    "    \n",
    "    best_test_log_loss = None\n",
    "    best_y_test_pred = None\n",
    "    best_y_test_pred_proba = None\n",
    "    best_y_test_real = None\n",
    "    \n",
    "    best_y_valid_pred = None\n",
    "    best_y_valid_pred_proba = np.zeros((0, 2))\n",
    "    best_y_valid_real = None\n",
    "    best_valid_log_loss = 100\n",
    "    \n",
    "    # Формирование обучающей и валидационной выборки\n",
    "    val_ind = int(X.shape[0] * valid)\n",
    "    X_valid, y_valid = X[:val_ind,:], y[:val_ind]\n",
    "    X, y = X[val_ind:,:], y[val_ind:]\n",
    "    \n",
    "    for clf_name, params, clf in get_classifier():\n",
    "        y_train_pred = np.array([])\n",
    "        y_train_real = np.array([])\n",
    "        y_train_pred_proba = np.zeros((0, 2))\n",
    "        \n",
    "        y_test_pred = np.array([])\n",
    "        y_test_pred_proba = np.zeros((0, 2))\n",
    "        y_test_real = np.array([])\n",
    "        \n",
    "        row_names.append(clf_name + ' ' + '_'.join(str(param) for param in params.values()))\n",
    "        \n",
    "        # Обучение модели на 10-кратной кросс-валидации\n",
    "        start_learning_time = time.time()\n",
    "        kfold_generator = StratifiedKFold(n_splits=kfolds)\n",
    "        for train_index, test_index in kfold_generator.split(X, y):\n",
    "            X_train = X[train_index]\n",
    "            y_train = y[train_index]\n",
    "\n",
    "            X_test = X[test_index]\n",
    "            y_test = y[test_index]\n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            y_train_real = np.concatenate((y_train_real, y_train))\n",
    "            y_train_pred = np.concatenate((y_train_pred, clf.predict(X_train)))\n",
    "            y_train_pred_proba = np.concatenate((y_train_pred_proba, clf.predict_proba(X_train)))\n",
    "            \n",
    "            y_test_real = np.concatenate((y_test_real, y_test))\n",
    "            y_test_pred = np.concatenate((y_test_pred, clf.predict(X_test)))\n",
    "            y_test_pred_proba = np.concatenate((y_test_pred_proba, clf.predict_proba(X_test)))\n",
    "            \n",
    "        \n",
    "        learning_time = time.time() - start_learning_time\n",
    "        \n",
    "        # Выбор лучшего алгоритма\n",
    "        if log_loss(y_valid, clf.predict_proba(X_valid)) < best_valid_log_loss:\n",
    "            best_clf_name = clf_name\n",
    "            best_params = params\n",
    "            \n",
    "            best_y_test_pred = y_test_pred\n",
    "            best_y_test_pred_proba = y_test_pred_proba\n",
    "            best_y_test_real = y_test_real\n",
    "            best_test_log_loss = log_loss(y_test_real, y_test_pred_proba)\n",
    "            \n",
    "            best_y_valid_pred = clf.predict(X_valid)\n",
    "            best_y_valid_pred_proba = clf.predict_proba(X_valid)\n",
    "            best_y_valid_real = y_valid\n",
    "            best_valid_log_loss = log_loss(y_valid, best_y_valid_pred_proba)\n",
    "        \n",
    "        # Расчет метрик для тренировочной, тестовой, валидационной выборки\n",
    "        train_acc = accuracy_score(y_train_real, y_train_pred)\n",
    "        train_auc = roc_auc_score(y_train_real, y_train_pred)\n",
    "        train_ll = log_loss(y_train_real, y_train_pred_proba)\n",
    "\n",
    "        test_acc = accuracy_score(y_test_real, y_test_pred)\n",
    "        test_auc = roc_auc_score(y_test_real, y_test_pred)\n",
    "        test_ll = log_loss(y_test_real, y_test_pred_proba)\n",
    "\n",
    "        valid_acc = accuracy_score(y_valid, clf.predict(X_valid))\n",
    "        valid_auc = roc_auc_score(y_valid, clf.predict(X_valid))\n",
    "        valid_ll = log_loss(y_valid, clf.predict_proba(X_valid))\n",
    "        \n",
    "        metrics = [\n",
    "            train_acc, train_auc, train_ll,\n",
    "            test_acc, test_auc, test_ll,valid_acc, valid_auc, valid_ll,\n",
    "            int(learning_time)\n",
    "        ]\n",
    "        \n",
    "        key_metrics_str = (\n",
    "            '{} {}\\ntrain: ({:.3f}, {:.3f}, {:.3f}) '\n",
    "            'test: ({:.3f}, {:.3f}, {:.3f}) '\n",
    "            'valid: ({:.3f}, {:.3f}, {:.3f}) - {}\\n').format(\n",
    "                clf_name, params, train_acc, train_auc, train_ll,\n",
    "                test_acc, test_auc, test_ll, valid_acc, valid_auc, valid_ll,\n",
    "                int(learning_time)\n",
    "        )\n",
    "        print(key_metrics_str)\n",
    "        \n",
    "        # Добавление расчетов в массив\n",
    "        if stats.shape[0]:\n",
    "            stats = np.vstack((stats, np.array(metrics)))\n",
    "        else:\n",
    "            stats = np.array(metrics)\n",
    "            \n",
    "        # Запись логов\n",
    "        with open(LOG_FILE, 'a', encoding='utf-8') as file:\n",
    "            file.write(key_metrics_str)\n",
    "        \n",
    "    best_algorithm_str = 'Лучший алгоритм {} с параметрами {} с valid_acc: {:.3f}, valid_ll: {:.3f}\\n'.format(\n",
    "        best_clf_name,\n",
    "        best_params,\n",
    "        accuracy_score(best_y_valid_real, best_y_valid_pred),\n",
    "        log_loss(best_y_valid_real, best_y_valid_pred_proba)\n",
    "    )\n",
    "    print(best_algorithm_str)\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as file:\n",
    "            file.write(best_algorithm_str)\n",
    "\n",
    "    print(classification_report(\n",
    "        best_y_valid_real,\n",
    "        best_y_valid_pred,\n",
    "        target_names=tuple(('Хороший', 'Плохой'))\n",
    "    ))\n",
    "    \n",
    "    time_consumed_str ='Выбор лучшего классификатора занял %s секунд' % int((time.time() - start_time))\n",
    "    print(time_consumed_str)\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as file:\n",
    "            file.write(time_consumed_str)\n",
    "    \n",
    "    result = pd.DataFrame(data=stats, columns=column_names, index=row_names)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_initial_dataset(data):\n",
    "    \"\"\"Предобработка данных\"\"\"\n",
    "    \n",
    "    # Удаление строк с пропущенными значениями\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Балансировка выборки\n",
    "    bad_cntr = data.loc[data.cntr_result == 0]\n",
    "    good_cntr = data.loc[data.cntr_result == 1].sample(bad_cntr.shape[0], random_state=RANDOM_SEED)\n",
    "    data = bad_cntr.append(good_cntr)\n",
    "    \n",
    "    # Перемена местами обозначений целевой переменной: \"0\" соответствует хорошему контракту, \"1\" - плохому \n",
    "    data.loc[data.cntr_result == 0, 'cntr_result'] = 2\n",
    "    data.loc[data.cntr_result == 1, 'cntr_result'] = 0\n",
    "    data.loc[data.cntr_result == 2, 'cntr_result'] = 1\n",
    "    \n",
    "     # Удаление неинформативных переменных (доля хороших контрактов по территории и ID контракта)\n",
    "    data.drop(['ter_good_cntr_share', 'cntrID'], inplace=True, axis=1)\n",
    "    # Удаление данных с пропущенной датой окончания контракта\n",
    "    data.drop(data[data.exec_date == -1].index, inplace=True)\n",
    "    \n",
    "    num_var = ['sup_cntr_num', 'sup_cntr_avg_price', 'org_cntr_num', 'org_cntr_avg_price', 'cntr_length']\n",
    "    num_var01 = [\n",
    "        'sup_good_cntr_share', 'sup_fed_cntr_share', 'sup_sub_cntr_share', \n",
    "        'sup_nun_cntr_share', 'sup_sim_price_share', 'org_good_cntr_share', 'org_fed_cntr_share', \n",
    "        'org_sub_cntr_share', 'org_nun_cntr_share', 'okpd_good_cntr_share']\n",
    "    cat_var = ['org_form', 'org_type', 'cntr_lvl', 'purch_type', 'okpd_2', 'okpd_3', 'okpd_4', 'quarter']\n",
    "    \n",
    "    # Создание дополнительных переменных, использующих первые 2, 3 и 4 символа ОКПД\n",
    "    data['okpd_2'] = data['okpd'].apply(lambda a: pd.Series(a[:2]))\n",
    "    data['okpd_3'] = data['okpd'].apply(lambda a: pd.Series(a[:3]))\n",
    "    data['okpd_4'] = data['okpd'].apply(lambda a: pd.Series(a[:4]))\n",
    "    \n",
    "    # Добавление переменной, отражающей квартал заключения контракта\n",
    "    sign_month = data['sign_date'].apply(lambda a: pd.Series(int(str(a)[4:6])))\n",
    "    data['quarter'] = sign_month.apply(lambda a: pd.Series((a - 1) // 3 + 1))\n",
    "    \n",
    "    # Рассчет длительности контракта\n",
    "    cntr_start = data['sign_date'].apply(lambda a: pd.Series(datetime.datetime.strptime(str(a), \"%Y%m%d\").date()))\n",
    "    cntr_end = data['exec_date'].apply(lambda a: pd.Series(datetime.datetime.strptime(str(a), \"%Y%m%d\").date()))\n",
    "    data['cntr_length'] = (cntr_end - cntr_start)[0].apply(lambda a: pd.Series(int(str(a).split()[0])))\n",
    "    \n",
    "    # Обработка выбросов для количественных переменных с нефиксированной областью значений\n",
    "    for nv in data[num_var]:\n",
    "        ulimit = np.percentile(data[nv].values, 99)\n",
    "        dlimit = np.percentile(data[nv].values, 1)\n",
    "        data.loc[data[nv] > ulimit, nv] = ulimit\n",
    "        data.loc[data[nv] < dlimit, nv] = dlimit\n",
    "    \n",
    "    for idx, nv in enumerate(('sup_cntr_avg_price', 'org_cntr_avg_price', 'cntr_length')):\n",
    "        if idx != 2:\n",
    "            ulimit = np.percentile(data[nv].values, 95)\n",
    "            data.loc[data[nv] > ulimit, nv] = ulimit  \n",
    "        else:   \n",
    "            dlimit = np.percentile(data[nv].values, 5)\n",
    "            data.loc[data[nv] < dlimit, nv] = dlimit\n",
    "    \n",
    "    # Логарифмирование количественных переменных       \n",
    "    for nv in data[num_var]:\n",
    "        data.loc[data[nv] < 1, nv] = 1\n",
    "        data[nv] = np.log(data[nv])\n",
    "    \n",
    "    # Шкалирование и центрирование количественных переменных\n",
    "    scaler = StandardScaler()\n",
    "    data[num_var] = scaler.fit_transform(data[num_var])\n",
    "    \n",
    "    # Обработка выбросов для количественных переменных с областью значений [0, 1]\n",
    "    for nv01 in num_var01:\n",
    "        ulimit = np.percentile(data[nv01].values, 99)\n",
    "        dlimit = np.percentile(data[nv01].values, 1)\n",
    "        data.loc[data[nv01] > ulimit, nv01] = ulimit\n",
    "        data.loc[data[nv01] < dlimit, nv01] = dlimit\n",
    "    \n",
    "    # Замена пропущенных значений для номинальных переменных\n",
    "    data.loc[(data.cntr_lvl == -1) | (data.cntr_lvl == 0), 'cntr_lvl'] = data.cntr_lvl.value_counts().index[0]\n",
    "    data.loc[(data.org_type == -1) | (data.org_type == 0), 'org_type'] = data.org_type.value_counts().index[0]\n",
    "    data.loc[(data.org_form == -1) | (data.org_form == 0), 'org_form'] = data.org_form.value_counts().index[0]\n",
    "    data.loc[(data.purch_type == -1) | (data.purch_type == 0), 'purch_type'] = data.purch_type.value_counts().index[0]\n",
    "    \n",
    "    # Группировка редки значений для номинальных переменных\n",
    "    for cv in cat_var:\n",
    "        cnt = data[cv].value_counts()\n",
    "        for val, count in zip(cnt.index, cnt.values):\n",
    "            if count / data.shape[0] <= 0.005:\n",
    "                data.loc[data[cv] == val, cv] = 'NEW'    \n",
    "    \n",
    "    # WoE кодирование номинальных переменных          \n",
    "    for cv in cat_var:\n",
    "        cnt = data[cv].value_counts()\n",
    "        for val, count in zip(cnt.index, cnt.values):\n",
    "            good_with_val = data.loc[(data.cntr_result == 1) & (data[cv] == val)].shape[0]\n",
    "            bad_with_val = data.loc[(data.cntr_result == 0) & (data[cv] == val)].shape[0]\n",
    "\n",
    "            p = good_with_val / data.loc[data.cntr_result == 1].shape[0]\n",
    "            q = bad_with_val / data.loc[data.cntr_result == 0].shape[0]\n",
    "            data.loc[data[cv] == val, cv] = round(np.log(p / q), 3)\n",
    "    \n",
    "    sample = data[num_var  + cat_var + num_var01 + ['cntr_result']]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.6 s, sys: 1.3 s, total: 58.9 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_csv('../data/tula_yarobl_contracts.csv', converters={'okpd': str})\n",
    "data = preprocess_initial_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=1, random_state=RANDOM_SEED)\n",
    "X = data.drop(['cntr_result'], axis=1).values\n",
    "y = data.cntr_result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRer {'C': 0.001, 'penalty': 'l1', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.746, 0.746, 0.533) test: (0.746, 0.746, 0.533) valid: (0.747, 0.747, 0.531) - 1\n",
      "\n",
      "LogRer {'C': 0.001, 'penalty': 'l1', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.746, 0.746, 0.533) test: (0.746, 0.746, 0.533) valid: (0.747, 0.748, 0.531) - 2\n",
      "\n",
      "LogRer {'C': 0.001, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.756, 0.756, 0.500) test: (0.756, 0.756, 0.500) valid: (0.754, 0.754, 0.497) - 2\n",
      "\n",
      "LogRer {'C': 0.001, 'penalty': 'l2', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.764, 0.764, 0.491) test: (0.764, 0.764, 0.492) valid: (0.761, 0.761, 0.489) - 2\n",
      "\n",
      "LogRer {'C': 0.01, 'penalty': 'l1', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.830, 0.830, 0.392) test: (0.829, 0.829, 0.392) valid: (0.831, 0.831, 0.391) - 9\n",
      "\n",
      "LogRer {'C': 0.01, 'penalty': 'l1', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.841, 0.841, 0.369) test: (0.841, 0.841, 0.369) valid: (0.843, 0.842, 0.369) - 2\n",
      "\n",
      "LogRer {'C': 0.01, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.802, 0.802, 0.435) test: (0.802, 0.802, 0.435) valid: (0.802, 0.802, 0.433) - 3\n",
      "\n",
      "LogRer {'C': 0.01, 'penalty': 'l2', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.816, 0.816, 0.413) test: (0.815, 0.815, 0.413) valid: (0.816, 0.817, 0.412) - 1\n",
      "\n",
      "LogRer {'C': 0.1, 'penalty': 'l1', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.851, 0.851, 0.348) test: (0.850, 0.850, 0.349) valid: (0.851, 0.851, 0.349) - 67\n",
      "\n",
      "LogRer {'C': 0.1, 'penalty': 'l1', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.852, 0.852, 0.347) test: (0.852, 0.852, 0.347) valid: (0.852, 0.852, 0.348) - 9\n",
      "\n",
      "LogRer {'C': 0.1, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.845, 0.845, 0.369) test: (0.845, 0.845, 0.369) valid: (0.846, 0.846, 0.368) - 4\n",
      "\n",
      "LogRer {'C': 0.1, 'penalty': 'l2', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.849, 0.849, 0.356) test: (0.849, 0.849, 0.357) valid: (0.849, 0.849, 0.356) - 7\n",
      "\n",
      "LogRer {'C': 1, 'penalty': 'l1', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.852, 0.852, 0.348) - 326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmitry/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRer {'C': 1, 'penalty': 'l1', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.853, 0.853, 0.348) - 16\n",
      "\n",
      "LogRer {'C': 1, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.851, 0.851, 0.347) test: (0.851, 0.851, 0.348) valid: (0.852, 0.852, 0.348) - 6\n",
      "\n",
      "LogRer {'C': 1, 'penalty': 'l2', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.852, 0.852, 0.346) test: (0.852, 0.852, 0.347) valid: (0.852, 0.852, 0.348) - 13\n",
      "\n",
      "LogRer {'C': 10, 'penalty': 'l1', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.853, 0.853, 0.348) - 45\n",
      "\n",
      "LogRer {'C': 10, 'penalty': 'l1', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.853, 0.853, 0.348) - 15\n",
      "\n",
      "LogRer {'C': 10, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.853, 0.853, 0.348) - 7\n",
      "\n",
      "LogRer {'C': 10, 'penalty': 'l2', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.853, 0.853, 0.348) - 18\n",
      "\n",
      "LogRer {'C': 100, 'penalty': 'l1', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.853, 0.853, 0.348) - 46\n",
      "\n",
      "LogRer {'C': 100, 'penalty': 'l1', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.853, 0.853, 0.348) - 15\n",
      "\n",
      "LogRer {'C': 100, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.853, 0.853, 0.348) - 10\n",
      "\n",
      "LogRer {'C': 100, 'penalty': 'l2', 'random_state': 42, 'solver': 'saga'}\n",
      "train: (0.853, 0.853, 0.346) test: (0.853, 0.853, 0.347) valid: (0.853, 0.853, 0.348) - 12\n",
      "\n",
      "RandForest {'max_depth': 3, 'n_estimators': 10, 'random_state': 42}\n",
      "train: (0.844, 0.844, 0.415) test: (0.844, 0.844, 0.416) valid: (0.842, 0.842, 0.416) - 1\n",
      "\n",
      "RandForest {'max_depth': 3, 'n_estimators': 100, 'random_state': 42}\n",
      "train: (0.841, 0.841, 0.413) test: (0.840, 0.840, 0.415) valid: (0.839, 0.839, 0.415) - 13\n",
      "\n",
      "RandForest {'max_depth': 3, 'n_estimators': 200, 'random_state': 42}\n",
      "train: (0.844, 0.844, 0.409) test: (0.842, 0.842, 0.411) valid: (0.842, 0.843, 0.411) - 26\n",
      "\n",
      "RandForest {'max_depth': 3, 'n_estimators': 400, 'random_state': 42}\n",
      "train: (0.843, 0.843, 0.412) test: (0.842, 0.842, 0.413) valid: (0.841, 0.841, 0.413) - 52\n",
      "\n",
      "RandForest {'max_depth': 3, 'n_estimators': 600, 'random_state': 42}\n",
      "train: (0.844, 0.844, 0.411) test: (0.843, 0.843, 0.413) valid: (0.842, 0.842, 0.413) - 81\n",
      "\n",
      "RandForest {'max_depth': 3, 'n_estimators': 800, 'random_state': 42}\n",
      "train: (0.845, 0.845, 0.412) test: (0.844, 0.843, 0.414) valid: (0.842, 0.843, 0.414) - 107\n",
      "\n",
      "RandForest {'max_depth': 3, 'n_estimators': 1000, 'random_state': 42}\n",
      "train: (0.845, 0.844, 0.412) test: (0.843, 0.843, 0.414) valid: (0.842, 0.843, 0.414) - 133\n",
      "\n",
      "RandForest {'max_depth': 4, 'n_estimators': 10, 'random_state': 42}\n",
      "train: (0.853, 0.853, 0.392) test: (0.852, 0.852, 0.394) valid: (0.846, 0.847, 0.395) - 2\n",
      "\n",
      "RandForest {'max_depth': 4, 'n_estimators': 100, 'random_state': 42}\n",
      "train: (0.864, 0.864, 0.382) test: (0.863, 0.863, 0.385) valid: (0.864, 0.865, 0.385) - 15\n",
      "\n",
      "RandForest {'max_depth': 4, 'n_estimators': 200, 'random_state': 42}\n",
      "train: (0.865, 0.865, 0.380) test: (0.863, 0.863, 0.382) valid: (0.865, 0.865, 0.383) - 33\n",
      "\n",
      "RandForest {'max_depth': 4, 'n_estimators': 400, 'random_state': 42}\n",
      "train: (0.865, 0.865, 0.381) test: (0.863, 0.863, 0.383) valid: (0.866, 0.866, 0.384) - 60\n",
      "\n",
      "RandForest {'max_depth': 4, 'n_estimators': 600, 'random_state': 42}\n",
      "train: (0.866, 0.866, 0.380) test: (0.865, 0.865, 0.383) valid: (0.867, 0.867, 0.383) - 100\n",
      "\n",
      "RandForest {'max_depth': 4, 'n_estimators': 800, 'random_state': 42}\n",
      "train: (0.867, 0.867, 0.380) test: (0.865, 0.865, 0.383) valid: (0.866, 0.867, 0.383) - 128\n",
      "\n",
      "RandForest {'max_depth': 4, 'n_estimators': 1000, 'random_state': 42}\n",
      "train: (0.867, 0.867, 0.381) test: (0.865, 0.865, 0.383) valid: (0.866, 0.867, 0.383) - 153\n",
      "\n",
      "RandForest {'max_depth': 6, 'n_estimators': 10, 'random_state': 42}\n",
      "train: (0.874, 0.874, 0.334) test: (0.870, 0.869, 0.340) valid: (0.873, 0.873, 0.346) - 2\n",
      "\n",
      "RandForest {'max_depth': 6, 'n_estimators': 100, 'random_state': 42}\n",
      "train: (0.878, 0.878, 0.330) test: (0.875, 0.875, 0.337) valid: (0.875, 0.876, 0.337) - 19\n",
      "\n",
      "RandForest {'max_depth': 6, 'n_estimators': 200, 'random_state': 42}\n",
      "train: (0.878, 0.878, 0.330) test: (0.875, 0.875, 0.336) valid: (0.877, 0.877, 0.336) - 38\n",
      "\n",
      "RandForest {'max_depth': 6, 'n_estimators': 400, 'random_state': 42}\n",
      "train: (0.878, 0.878, 0.330) test: (0.875, 0.875, 0.337) valid: (0.877, 0.877, 0.337) - 76\n",
      "\n",
      "RandForest {'max_depth': 6, 'n_estimators': 600, 'random_state': 42}\n",
      "train: (0.878, 0.878, 0.330) test: (0.875, 0.875, 0.336) valid: (0.877, 0.877, 0.336) - 116\n",
      "\n",
      "RandForest {'max_depth': 6, 'n_estimators': 800, 'random_state': 42}\n",
      "train: (0.878, 0.878, 0.329) test: (0.875, 0.875, 0.336) valid: (0.877, 0.877, 0.336) - 162\n",
      "\n",
      "RandForest {'max_depth': 6, 'n_estimators': 1000, 'random_state': 42}\n",
      "train: (0.878, 0.878, 0.330) test: (0.875, 0.875, 0.336) valid: (0.876, 0.877, 0.336) - 202\n",
      "\n",
      "RandForest {'max_depth': 8, 'n_estimators': 10, 'random_state': 42}\n",
      "train: (0.888, 0.888, 0.292) test: (0.878, 0.878, 0.308) valid: (0.884, 0.884, 0.308) - 3\n",
      "\n",
      "RandForest {'max_depth': 8, 'n_estimators': 100, 'random_state': 42}\n",
      "train: (0.891, 0.890, 0.287) test: (0.882, 0.882, 0.303) valid: (0.883, 0.884, 0.303) - 23\n",
      "\n",
      "RandForest {'max_depth': 8, 'n_estimators': 200, 'random_state': 42}\n",
      "train: (0.890, 0.890, 0.286) test: (0.882, 0.882, 0.301) valid: (0.884, 0.884, 0.301) - 49\n",
      "\n",
      "RandForest {'max_depth': 8, 'n_estimators': 400, 'random_state': 42}\n",
      "train: (0.891, 0.891, 0.286) test: (0.882, 0.882, 0.302) valid: (0.883, 0.884, 0.302) - 100\n",
      "\n",
      "RandForest {'max_depth': 8, 'n_estimators': 600, 'random_state': 42}\n",
      "train: (0.891, 0.891, 0.286) test: (0.882, 0.882, 0.301) valid: (0.883, 0.884, 0.301) - 153\n",
      "\n",
      "RandForest {'max_depth': 8, 'n_estimators': 800, 'random_state': 42}\n",
      "train: (0.891, 0.891, 0.286) test: (0.882, 0.882, 0.301) valid: (0.883, 0.884, 0.302) - 192\n",
      "\n",
      "RandForest {'max_depth': 8, 'n_estimators': 1000, 'random_state': 42}\n",
      "train: (0.891, 0.890, 0.286) test: (0.882, 0.882, 0.301) valid: (0.883, 0.883, 0.302) - 249\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.882, 0.882, 0.266) test: (0.879, 0.879, 0.274) valid: (0.882, 0.883, 0.274) - 19\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.882, 0.881, 0.267) test: (0.878, 0.878, 0.274) valid: (0.880, 0.881, 0.276) - 18\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.881, 0.881, 0.269) test: (0.879, 0.879, 0.276) valid: (0.880, 0.881, 0.276) - 15\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.892, 0.892, 0.246) test: (0.885, 0.885, 0.261) valid: (0.887, 0.888, 0.264) - 44\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.891, 0.891, 0.247) test: (0.885, 0.884, 0.262) valid: (0.886, 0.887, 0.265) - 38\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.889, 0.889, 0.251) test: (0.884, 0.884, 0.264) valid: (0.885, 0.886, 0.266) - 31\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.907, 0.907, 0.220) test: (0.892, 0.892, 0.248) valid: (0.893, 0.893, 0.253) - 78\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.906, 0.906, 0.221) test: (0.893, 0.893, 0.249) valid: (0.895, 0.895, 0.253) - 73\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.904, 0.904, 0.226) test: (0.891, 0.891, 0.251) valid: (0.893, 0.893, 0.256) - 64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.924, 0.924, 0.188) test: (0.900, 0.900, 0.238) valid: (0.896, 0.897, 0.243) - 156\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.924, 0.924, 0.188) test: (0.900, 0.900, 0.238) valid: (0.899, 0.900, 0.242) - 158\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.921, 0.921, 0.194) test: (0.898, 0.898, 0.239) valid: (0.898, 0.898, 0.245) - 144\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.931, 0.931, 0.176) test: (0.903, 0.903, 0.235) valid: (0.899, 0.899, 0.240) - 209\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.931, 0.931, 0.176) test: (0.902, 0.902, 0.234) valid: (0.900, 0.901, 0.240) - 211\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.928, 0.927, 0.182) test: (0.901, 0.901, 0.235) valid: (0.899, 0.900, 0.242) - 172\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.890, 0.890, 0.249) test: (0.883, 0.883, 0.263) valid: (0.886, 0.887, 0.266) - 33\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.889, 0.889, 0.251) test: (0.883, 0.883, 0.265) valid: (0.884, 0.885, 0.267) - 33\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.887, 0.886, 0.254) test: (0.882, 0.882, 0.267) valid: (0.883, 0.883, 0.269) - 26\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.906, 0.906, 0.220) test: (0.892, 0.892, 0.248) valid: (0.892, 0.893, 0.253) - 52\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.905, 0.905, 0.222) test: (0.892, 0.892, 0.249) valid: (0.892, 0.893, 0.253) - 48\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.902, 0.902, 0.227) test: (0.891, 0.891, 0.252) valid: (0.890, 0.891, 0.256) - 41\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.927, 0.927, 0.183) test: (0.901, 0.900, 0.234) valid: (0.899, 0.899, 0.240) - 105\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.927, 0.927, 0.184) test: (0.901, 0.901, 0.235) valid: (0.900, 0.900, 0.241) - 98\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.923, 0.923, 0.191) test: (0.899, 0.899, 0.237) valid: (0.899, 0.900, 0.244) - 84\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.952, 0.951, 0.138) test: (0.908, 0.908, 0.223) valid: (0.905, 0.906, 0.230) - 212\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.951, 0.951, 0.139) test: (0.908, 0.908, 0.223) valid: (0.905, 0.906, 0.231) - 184\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.947, 0.947, 0.147) test: (0.906, 0.906, 0.225) valid: (0.903, 0.903, 0.234) - 159\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.959, 0.959, 0.123) test: (0.910, 0.910, 0.222) valid: (0.907, 0.908, 0.227) - 253\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.960, 0.960, 0.123) test: (0.909, 0.909, 0.221) valid: (0.905, 0.906, 0.229) - 230\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.956, 0.956, 0.131) test: (0.908, 0.908, 0.222) valid: (0.905, 0.905, 0.231) - 224\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.914, 0.914, 0.208) test: (0.895, 0.895, 0.243) valid: (0.896, 0.897, 0.248) - 40\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.911, 0.911, 0.213) test: (0.894, 0.894, 0.246) valid: (0.895, 0.895, 0.248) - 35\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.908, 0.908, 0.217) test: (0.893, 0.893, 0.248) valid: (0.892, 0.892, 0.250) - 32\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.940, 0.940, 0.160) test: (0.904, 0.904, 0.225) valid: (0.901, 0.901, 0.232) - 77\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.940, 0.940, 0.162) test: (0.904, 0.904, 0.226) valid: (0.904, 0.904, 0.230) - 73\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.933, 0.933, 0.174) test: (0.902, 0.902, 0.232) valid: (0.900, 0.900, 0.237) - 66\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.970, 0.970, 0.104) test: (0.913, 0.913, 0.213) valid: (0.908, 0.909, 0.223) - 161\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.969, 0.969, 0.107) test: (0.913, 0.913, 0.213) valid: (0.909, 0.910, 0.217) - 151\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.962, 0.962, 0.121) test: (0.910, 0.910, 0.218) valid: (0.907, 0.908, 0.226) - 135\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.993, 0.993, 0.052) test: (0.918, 0.918, 0.215) valid: (0.912, 0.913, 0.225) - 329\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.992, 0.992, 0.054) test: (0.916, 0.916, 0.214) valid: (0.916, 0.916, 0.217) - 295\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.987, 0.987, 0.068) test: (0.916, 0.916, 0.215) valid: (0.910, 0.910, 0.222) - 265\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.997, 0.997, 0.039) test: (0.919, 0.919, 0.220) valid: (0.915, 0.915, 0.230) - 412\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.996, 0.996, 0.041) test: (0.917, 0.917, 0.219) valid: (0.917, 0.917, 0.221) - 370\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.992, 0.992, 0.053) test: (0.916, 0.916, 0.217) valid: (0.911, 0.912, 0.226) - 344\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.941, 0.940, 0.161) test: (0.903, 0.903, 0.227) valid: (0.903, 0.903, 0.229) - 58\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.938, 0.938, 0.164) test: (0.904, 0.904, 0.229) valid: (0.904, 0.904, 0.230) - 53\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.932, 0.932, 0.174) test: (0.901, 0.901, 0.232) valid: (0.903, 0.903, 0.234) - 44\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.973, 0.973, 0.100) test: (0.913, 0.913, 0.212) valid: (0.909, 0.910, 0.215) - 110\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.972, 0.972, 0.103) test: (0.913, 0.913, 0.211) valid: (0.910, 0.911, 0.215) - 105\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.965, 0.964, 0.118) test: (0.910, 0.910, 0.217) valid: (0.910, 0.911, 0.221) - 90\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.995, 0.995, 0.047) test: (0.917, 0.917, 0.211) valid: (0.914, 0.914, 0.216) - 235\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.995, 0.995, 0.049) test: (0.918, 0.918, 0.209) valid: (0.915, 0.916, 0.214) - 204\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.989, 0.989, 0.064) test: (0.916, 0.916, 0.210) valid: (0.913, 0.913, 0.216) - 179\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.999, 0.999, 0.017) test: (0.920, 0.920, 0.234) valid: (0.917, 0.917, 0.238) - 452\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.999, 0.999, 0.018) test: (0.920, 0.920, 0.229) valid: (0.917, 0.918, 0.234) - 419\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.998, 0.998, 0.027) test: (0.919, 0.919, 0.223) valid: (0.918, 0.918, 0.229) - 348\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (1.000, 1.000, 0.013) test: (0.920, 0.920, 0.245) valid: (0.918, 0.918, 0.249) - 608\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (1.000, 1.000, 0.013) test: (0.919, 0.919, 0.240) valid: (0.918, 0.918, 0.246) - 569\n",
      "\n",
      "XGBoost {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.999, 0.999, 0.019) test: (0.919, 0.919, 0.231) valid: (0.918, 0.918, 0.237) - 467\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.882, 0.882, 0.266) test: (0.879, 0.879, 0.274) valid: (0.882, 0.883, 0.274) - 19\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.882, 0.881, 0.267) test: (0.878, 0.878, 0.274) valid: (0.880, 0.881, 0.276) - 17\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.881, 0.881, 0.269) test: (0.879, 0.879, 0.276) valid: (0.880, 0.881, 0.276) - 16\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.892, 0.892, 0.246) test: (0.885, 0.885, 0.261) valid: (0.887, 0.888, 0.264) - 37\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.891, 0.891, 0.247) test: (0.885, 0.884, 0.262) valid: (0.886, 0.887, 0.265) - 35\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.889, 0.889, 0.251) test: (0.884, 0.884, 0.264) valid: (0.885, 0.886, 0.266) - 31\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.907, 0.907, 0.220) test: (0.892, 0.892, 0.248) valid: (0.893, 0.893, 0.253) - 84\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.906, 0.906, 0.221) test: (0.893, 0.893, 0.249) valid: (0.895, 0.895, 0.253) - 75\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.904, 0.904, 0.226) test: (0.891, 0.891, 0.251) valid: (0.893, 0.893, 0.256) - 63\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.924, 0.924, 0.188) test: (0.900, 0.900, 0.238) valid: (0.896, 0.897, 0.243) - 165\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.924, 0.924, 0.188) test: (0.900, 0.900, 0.238) valid: (0.899, 0.900, 0.242) - 137\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.921, 0.921, 0.194) test: (0.898, 0.898, 0.239) valid: (0.898, 0.898, 0.245) - 125\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.931, 0.931, 0.176) test: (0.903, 0.903, 0.235) valid: (0.899, 0.899, 0.240) - 203\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.931, 0.931, 0.176) test: (0.902, 0.902, 0.234) valid: (0.900, 0.901, 0.240) - 184\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.928, 0.927, 0.182) test: (0.901, 0.901, 0.235) valid: (0.899, 0.900, 0.242) - 160\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.890, 0.890, 0.249) test: (0.883, 0.883, 0.263) valid: (0.886, 0.887, 0.266) - 26\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.889, 0.889, 0.251) test: (0.883, 0.883, 0.265) valid: (0.884, 0.885, 0.267) - 23\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.887, 0.886, 0.254) test: (0.882, 0.882, 0.267) valid: (0.883, 0.883, 0.269) - 20\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.906, 0.906, 0.220) test: (0.892, 0.892, 0.248) valid: (0.892, 0.893, 0.253) - 51\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.905, 0.905, 0.222) test: (0.892, 0.892, 0.249) valid: (0.892, 0.893, 0.253) - 47\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.902, 0.902, 0.227) test: (0.891, 0.891, 0.252) valid: (0.890, 0.891, 0.256) - 43\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.927, 0.927, 0.183) test: (0.901, 0.900, 0.234) valid: (0.899, 0.899, 0.240) - 111\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.927, 0.927, 0.184) test: (0.901, 0.901, 0.235) valid: (0.900, 0.900, 0.241) - 95\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.923, 0.923, 0.191) test: (0.899, 0.899, 0.237) valid: (0.899, 0.900, 0.244) - 85\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.952, 0.951, 0.138) test: (0.908, 0.908, 0.223) valid: (0.905, 0.906, 0.230) - 217\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.951, 0.951, 0.139) test: (0.908, 0.908, 0.223) valid: (0.905, 0.906, 0.231) - 205\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.947, 0.947, 0.147) test: (0.906, 0.906, 0.225) valid: (0.903, 0.903, 0.234) - 171\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.959, 0.959, 0.123) test: (0.910, 0.910, 0.222) valid: (0.907, 0.908, 0.227) - 272\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.960, 0.960, 0.123) test: (0.909, 0.909, 0.221) valid: (0.905, 0.906, 0.229) - 245\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.956, 0.956, 0.131) test: (0.908, 0.908, 0.222) valid: (0.905, 0.905, 0.231) - 222\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.914, 0.914, 0.208) test: (0.895, 0.895, 0.243) valid: (0.896, 0.897, 0.248) - 42\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.911, 0.911, 0.213) test: (0.894, 0.894, 0.246) valid: (0.895, 0.895, 0.248) - 37\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.908, 0.908, 0.217) test: (0.893, 0.893, 0.248) valid: (0.892, 0.892, 0.250) - 34\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.940, 0.940, 0.160) test: (0.904, 0.904, 0.225) valid: (0.901, 0.901, 0.232) - 81\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.940, 0.940, 0.162) test: (0.904, 0.904, 0.226) valid: (0.904, 0.904, 0.230) - 70\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.933, 0.933, 0.174) test: (0.902, 0.902, 0.232) valid: (0.900, 0.900, 0.237) - 65\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.970, 0.970, 0.104) test: (0.913, 0.913, 0.213) valid: (0.908, 0.909, 0.223) - 169\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.969, 0.969, 0.107) test: (0.913, 0.913, 0.213) valid: (0.909, 0.910, 0.217) - 149\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.962, 0.962, 0.121) test: (0.910, 0.910, 0.218) valid: (0.907, 0.908, 0.226) - 137\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.993, 0.993, 0.052) test: (0.918, 0.918, 0.215) valid: (0.912, 0.913, 0.225) - 327\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.992, 0.992, 0.054) test: (0.916, 0.916, 0.214) valid: (0.916, 0.916, 0.217) - 310\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.987, 0.987, 0.068) test: (0.916, 0.916, 0.215) valid: (0.910, 0.910, 0.222) - 266\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.997, 0.997, 0.039) test: (0.919, 0.919, 0.220) valid: (0.915, 0.915, 0.230) - 435\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.996, 0.996, 0.041) test: (0.917, 0.917, 0.219) valid: (0.917, 0.917, 0.221) - 404\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.992, 0.992, 0.053) test: (0.916, 0.916, 0.217) valid: (0.911, 0.912, 0.226) - 353\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.941, 0.940, 0.161) test: (0.903, 0.903, 0.227) valid: (0.903, 0.903, 0.229) - 56\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.938, 0.938, 0.164) test: (0.904, 0.904, 0.229) valid: (0.904, 0.904, 0.230) - 58\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.932, 0.932, 0.174) test: (0.901, 0.901, 0.232) valid: (0.903, 0.903, 0.234) - 47\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.973, 0.973, 0.100) test: (0.913, 0.913, 0.212) valid: (0.909, 0.910, 0.215) - 141\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.972, 0.972, 0.103) test: (0.913, 0.913, 0.211) valid: (0.910, 0.911, 0.215) - 116\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.965, 0.964, 0.118) test: (0.910, 0.910, 0.217) valid: (0.910, 0.911, 0.221) - 104\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.995, 0.995, 0.047) test: (0.917, 0.917, 0.211) valid: (0.914, 0.914, 0.216) - 241\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.995, 0.995, 0.049) test: (0.918, 0.918, 0.209) valid: (0.915, 0.916, 0.214) - 207\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.989, 0.989, 0.064) test: (0.916, 0.916, 0.210) valid: (0.913, 0.913, 0.216) - 188\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.999, 0.999, 0.017) test: (0.920, 0.920, 0.234) valid: (0.917, 0.917, 0.238) - 475\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.999, 0.999, 0.018) test: (0.920, 0.920, 0.229) valid: (0.917, 0.918, 0.234) - 429\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.998, 0.998, 0.027) test: (0.919, 0.919, 0.223) valid: (0.918, 0.918, 0.229) - 347\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (1.000, 1.000, 0.013) test: (0.920, 0.920, 0.245) valid: (0.918, 0.918, 0.249) - 533\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (1.000, 1.000, 0.013) test: (0.919, 0.919, 0.240) valid: (0.918, 0.918, 0.246) - 480\n",
      "\n",
      "XGBoost {'eta': 0.1, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.999, 0.999, 0.019) test: (0.919, 0.919, 0.231) valid: (0.918, 0.918, 0.237) - 419\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.882, 0.882, 0.266) test: (0.879, 0.879, 0.274) valid: (0.882, 0.883, 0.274) - 21\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.882, 0.881, 0.267) test: (0.878, 0.878, 0.274) valid: (0.880, 0.881, 0.276) - 17\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.881, 0.881, 0.269) test: (0.879, 0.879, 0.276) valid: (0.880, 0.881, 0.276) - 15\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.892, 0.892, 0.246) test: (0.885, 0.885, 0.261) valid: (0.887, 0.888, 0.264) - 36\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.891, 0.891, 0.247) test: (0.885, 0.884, 0.262) valid: (0.886, 0.887, 0.265) - 33\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.889, 0.889, 0.251) test: (0.884, 0.884, 0.264) valid: (0.885, 0.886, 0.266) - 29\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.907, 0.907, 0.220) test: (0.892, 0.892, 0.248) valid: (0.893, 0.893, 0.253) - 84\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.906, 0.906, 0.221) test: (0.893, 0.893, 0.249) valid: (0.895, 0.895, 0.253) - 85\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.904, 0.904, 0.226) test: (0.891, 0.891, 0.251) valid: (0.893, 0.893, 0.256) - 65\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.924, 0.924, 0.188) test: (0.900, 0.900, 0.238) valid: (0.896, 0.897, 0.243) - 149\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.924, 0.924, 0.188) test: (0.900, 0.900, 0.238) valid: (0.899, 0.900, 0.242) - 138\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.921, 0.921, 0.194) test: (0.898, 0.898, 0.239) valid: (0.898, 0.898, 0.245) - 121\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.931, 0.931, 0.176) test: (0.903, 0.903, 0.235) valid: (0.899, 0.899, 0.240) - 196\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.931, 0.931, 0.176) test: (0.902, 0.902, 0.234) valid: (0.900, 0.901, 0.240) - 180\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 3, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.928, 0.927, 0.182) test: (0.901, 0.901, 0.235) valid: (0.899, 0.900, 0.242) - 162\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.890, 0.890, 0.249) test: (0.883, 0.883, 0.263) valid: (0.886, 0.887, 0.266) - 29\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.889, 0.889, 0.251) test: (0.883, 0.883, 0.265) valid: (0.884, 0.885, 0.267) - 22\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.887, 0.886, 0.254) test: (0.882, 0.882, 0.267) valid: (0.883, 0.883, 0.269) - 20\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.906, 0.906, 0.220) test: (0.892, 0.892, 0.248) valid: (0.892, 0.893, 0.253) - 49\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.905, 0.905, 0.222) test: (0.892, 0.892, 0.249) valid: (0.892, 0.893, 0.253) - 44\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.902, 0.902, 0.227) test: (0.891, 0.891, 0.252) valid: (0.890, 0.891, 0.256) - 40\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.927, 0.927, 0.183) test: (0.901, 0.900, 0.234) valid: (0.899, 0.899, 0.240) - 99\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.927, 0.927, 0.184) test: (0.901, 0.901, 0.235) valid: (0.900, 0.900, 0.241) - 90\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.923, 0.923, 0.191) test: (0.899, 0.899, 0.237) valid: (0.899, 0.900, 0.244) - 80\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.952, 0.951, 0.138) test: (0.908, 0.908, 0.223) valid: (0.905, 0.906, 0.230) - 200\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.951, 0.951, 0.139) test: (0.908, 0.908, 0.223) valid: (0.905, 0.906, 0.231) - 187\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.947, 0.947, 0.147) test: (0.906, 0.906, 0.225) valid: (0.903, 0.903, 0.234) - 170\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.959, 0.959, 0.123) test: (0.910, 0.910, 0.222) valid: (0.907, 0.908, 0.227) - 284\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.960, 0.960, 0.123) test: (0.909, 0.909, 0.221) valid: (0.905, 0.906, 0.229) - 267\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 4, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.956, 0.956, 0.131) test: (0.908, 0.908, 0.222) valid: (0.905, 0.905, 0.231) - 219\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.914, 0.914, 0.208) test: (0.895, 0.895, 0.243) valid: (0.896, 0.897, 0.248) - 42\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.911, 0.911, 0.213) test: (0.894, 0.894, 0.246) valid: (0.895, 0.895, 0.248) - 41\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.908, 0.908, 0.217) test: (0.893, 0.893, 0.248) valid: (0.892, 0.892, 0.250) - 32\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.940, 0.940, 0.160) test: (0.904, 0.904, 0.225) valid: (0.901, 0.901, 0.232) - 76\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.940, 0.940, 0.162) test: (0.904, 0.904, 0.226) valid: (0.904, 0.904, 0.230) - 70\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.933, 0.933, 0.174) test: (0.902, 0.902, 0.232) valid: (0.900, 0.900, 0.237) - 63\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.970, 0.970, 0.104) test: (0.913, 0.913, 0.213) valid: (0.908, 0.909, 0.223) - 180\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.969, 0.969, 0.107) test: (0.913, 0.913, 0.213) valid: (0.909, 0.910, 0.217) - 140\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.962, 0.962, 0.121) test: (0.910, 0.910, 0.218) valid: (0.907, 0.908, 0.226) - 121\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.993, 0.993, 0.052) test: (0.918, 0.918, 0.215) valid: (0.912, 0.913, 0.225) - 307\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.992, 0.992, 0.054) test: (0.916, 0.916, 0.214) valid: (0.916, 0.916, 0.217) - 277\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.987, 0.987, 0.068) test: (0.916, 0.916, 0.215) valid: (0.910, 0.910, 0.222) - 242\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.997, 0.997, 0.039) test: (0.919, 0.919, 0.220) valid: (0.915, 0.915, 0.230) - 383\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.996, 0.996, 0.041) test: (0.917, 0.917, 0.219) valid: (0.917, 0.917, 0.221) - 337\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 6, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.992, 0.992, 0.053) test: (0.916, 0.916, 0.217) valid: (0.911, 0.912, 0.226) - 294\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.941, 0.940, 0.161) test: (0.903, 0.903, 0.227) valid: (0.903, 0.903, 0.229) - 50\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.938, 0.938, 0.164) test: (0.904, 0.904, 0.229) valid: (0.904, 0.904, 0.230) - 47\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 100, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.932, 0.932, 0.174) test: (0.901, 0.901, 0.232) valid: (0.903, 0.903, 0.234) - 43\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.973, 0.973, 0.100) test: (0.913, 0.913, 0.212) valid: (0.909, 0.910, 0.215) - 103\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.972, 0.972, 0.103) test: (0.913, 0.913, 0.211) valid: (0.910, 0.911, 0.215) - 94\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 200, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.965, 0.964, 0.118) test: (0.910, 0.910, 0.217) valid: (0.910, 0.911, 0.221) - 84\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.995, 0.995, 0.047) test: (0.917, 0.917, 0.211) valid: (0.914, 0.914, 0.216) - 207\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.995, 0.995, 0.049) test: (0.918, 0.918, 0.209) valid: (0.915, 0.916, 0.214) - 189\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.989, 0.989, 0.064) test: (0.916, 0.916, 0.210) valid: (0.913, 0.913, 0.216) - 166\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (0.999, 0.999, 0.017) test: (0.920, 0.920, 0.234) valid: (0.917, 0.917, 0.238) - 419\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 800, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (0.999, 0.999, 0.018) test: (0.920, 0.920, 0.229) valid: (0.917, 0.918, 0.234) - 376\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 800, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.998, 0.998, 0.027) test: (0.919, 0.919, 0.223) valid: (0.918, 0.918, 0.229) - 328\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.7}\n",
      "train: (1.000, 1.000, 0.013) test: (0.920, 0.920, 0.245) valid: (0.918, 0.918, 0.249) - 525\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 1000, 'random_state': 42, 'subsample': 0.85}\n",
      "train: (1.000, 1.000, 0.013) test: (0.919, 0.919, 0.240) valid: (0.918, 0.918, 0.246) - 473\n",
      "\n",
      "XGBoost {'eta': 0.3, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 1000, 'random_state': 42, 'subsample': 1}\n",
      "train: (0.999, 0.999, 0.019) test: (0.919, 0.919, 0.231) valid: (0.918, 0.918, 0.237) - 418\n",
      "\n",
      "Лучший алгоритм XGBoost с параметрами {'eta': 0.03, 'logging_level': 'Silent', 'max_depth': 8, 'n_estimators': 400, 'random_state': 42, 'subsample': 0.85} с valid_acc: 0.915, valid_ll: 0.214\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Хороший       0.94      0.89      0.91      3804\n",
      "     Плохой       0.89      0.94      0.92      3704\n",
      "\n",
      "avg / total       0.92      0.92      0.92      7508\n",
      "\n",
      "Выбор лучшего классификатора занял 32420 секунд\n"
     ]
    }
   ],
   "source": [
    "result = choose_best_classifier(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Экспорт результатов\n",
    "result.to_csv(path_or_buf='parameter_tuning.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
